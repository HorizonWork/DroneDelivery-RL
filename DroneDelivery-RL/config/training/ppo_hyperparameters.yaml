# PPO Hyperparameters - Exact match with Table 2 from final report
ppo:
  # Core PPO parameters
  learning_rate: 3.0e-4              # Adam optimizer step size
  rollout_length: 2048               # Environment steps per update (n_steps) 
  batch_size: 64                     # Size of mini-batches for gradient updates
  epochs_per_update: 10              # Number of passes over each batch
  clip_range: 0.2                    # PPO clipping parameter (ε)
  discount_factor: 0.99              # Weighting of future rewards (γ)
  gae_parameter: 0.95                # Generalized advantage estimation decay (λ)
  entropy_coefficient: 0.01          # Encourages exploration
  
  # Network architecture
  hidden_layers: [256, 128, 64]      # Three hidden layers in actor and critic networks
  activation_function: "tanh"        # Non-linearity after each hidden layer
  
  # Training settings
  total_timesteps: 5000000           # Total training timesteps (5M)
  max_grad_norm: 0.5                 # Gradient clipping
  value_loss_coefficient: 0.5        # Value function loss weight
  
  # Advanced settings
  normalize_advantage: true          # Normalize advantage estimates
  use_gae: true                     # Use Generalized Advantage Estimation
  target_kl: null                   # Target KL divergence (None = no limit)
  
# Optimizer settings
optimizer:
  type: "Adam"
  learning_rate: 3.0e-4
  eps: 1.0e-5
  weight_decay: 0.0
  
# Learning rate scheduling
lr_schedule:
  type: "constant"                   # constant, linear, cosine
  initial_lr: 3.0e-4
  final_lr: 1.0e-5
  
# Logging and saving
logging:
  log_interval: 100                  # Log every N updates
  save_interval: 10000               # Save model every N timesteps
  eval_interval: 50000               # Evaluate every N timesteps
  tensorboard: true                  # Enable TensorBoard logging
  wandb: false                       # Enable Weights & Biases logging
