# H∆Ø·ªöNG D·∫™N HU·∫§N LUY·ªÜN M√î H√åNH
## DroneDelivery-RL PPO Training Guide

---

## üéØ **M·ª§C TI√äU**

H∆∞·ªõng d·∫´n chi ti·∫øt qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh PPO cho h·ªá th·ªëng ƒëi·ªÅu h∆∞·ªõng drone:
- Hu·∫•n luy·ªán curriculum learning (1 t·∫ßng ‚Üí 2 t·∫ßng ‚Üí 5 t·∫ßng)
- C·∫•u h√¨nh si√™u tham s·ªë theo Table 2
- Theo d√µi qu√° tr√¨nh hu·∫•n luy·ªán
- L∆∞u v√† qu·∫£n l√Ω checkpoints

---

## üìã **Y√äU C·∫¶U TR∆Ø·ªöC HU·∫§N LUY·ªÜN**

### 1. M√¥i tr∆∞·ªùng ƒë√£ c√†i ƒë·∫∑t
```bash
# Ki·ªÉm tra c√†i ƒë·∫∑t
python scripts/setup/verify_installation.py

# ƒê·∫£m b·∫£o c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
pip list | grep -E "(torch|gymnasium|stable-baselines3)"
```

### 2. T√†i nguy√™n h·ªá th·ªëng
- **GPU**: RTX 3070+ (khuy·∫øn ngh·ªã) ho·∫∑c CPU 8+ cores
- **RAM**: 16GB+ (32GB khuy·∫øn ngh·ªã)
- **Storage**: 50GB+ cho checkpoints v√† logs
- **Th·ªùi gian**: 8-12 gi·ªù cho 5 tri·ªáu timestep

---

## üöÄ **QUY TR√åNH HU·∫§N LUY·ªÜN C∆† B·∫¢N**

### 1. Hu·∫•n luy·ªán to√†n b·ªô (Full curriculum)
```bash
# Hu·∫•n luy·ªán PPO v·ªõi curriculum learning ho√†n ch·ªânh
python scripts/training/train_full_curriculum.py

# V·ªõi c√°c t√πy ch·ªçn c·ª• th·ªÉ
python scripts/training/train_full_curriculum.py \
    --config config/training/ppo_hyperparameters.yaml \
    --output-dir models/checkpoints \
    --log-dir logs/training \
    --total-timesteps 50000 \
    --name curriculum_training
```

### 2. Hu·∫•n luy·ªán t·ª´ng giai ƒëo·∫°n
```bash
# Giai ƒëo·∫°n 1: 1 t·∫ßng (1 tri·ªáu timestep)
python scripts/training/train_phase.py \
    --phase 1 \
    --timesteps 10000 \
    --output-dir models/checkpoints/phase_1

# Giai ƒëo·∫°n 2: 2 t·∫ßng (2 tri·ªáu timestep)  
# Resume t·ª´ phase 1
python scripts/training/train_phase.py \
    --phase 2 \
    --timesteps 200000 \
    --resume models/checkpoints/phase_1/final_model.pt \
    --output-dir models/checkpoints/phase_2

# Giai ƒëo·∫°n 3: 5 t·∫ßng (2 tri·ªáu timestep)
# Resume t·ª´ phase 2
python scripts/training/train_phase.py \
    --phase 3 \
    --timesteps 200000 \
    --resume models/checkpoints/phase_2/final_model.pt \
    --output-dir models/checkpoints/phase_3
```

---

## ‚öôÔ∏è **C·∫§U H√åNH HU·∫§N LUY·ªÜN**

### 1. Si√™u tham s·ªë ch√≠nh (Table 2)
```yaml
# config/training/ppo_hyperparameters.yaml
ppo:
  learning_rate: 3.0e-4           # Adam optimizer step size
  rollout_length: 2048            # Environment steps per update
  batch_size: 64                  # Size of mini-batches
  epochs_per_update: 10           # Number of passes over batch
  clip_range: 0.2                 # PPO clipping parameter
  discount_factor: 0.99           # Future rewards weighting
  gae_lambda: 0.95                # GAE parameter
  entropy_coefficient: 0.01       # Exploration encouragement
  value_loss_coefficient: 0.5     # Value loss weight
  max_grad_norm: 0.5              # Gradient clipping

model:
  hidden_sizes: [256, 128, 64]    # Network architecture
  activation: "tanh"              # Activation function
```

### 2. C·∫•u h√¨nh m√¥i tr∆∞·ªùng hu·∫•n luy·ªán
```yaml
# config/training/environment_config.yaml
environment:
  # Curriculum configuration
  curriculum:
    phases:
      - name: "single_floor"
        floors: 1
        timesteps: 1000000
        obstacles: ["static"]
      
      - name: "two_floors" 
        floors: 2
        timesteps: 200000
        obstacles: ["static", "moving"]
      
      - name: "five_floors"
        floors: 5
        timesteps: 2000000
        obstacles: ["static", "moving", "dynamic"]

  # Training-specific settings
  reward:
    energy_efficiency_weight: 0.3
    success_weight: 0.5
    collision_penalty: 1000.0
    time_penalty: 0.1
  
  observation:
    normalize: true
    stack_frames: 1
    
  action:
    clip_actions: true
    scale_actions: true
```

---

## üìä **THEO D√ïI HU·∫§N LUY·ªÜN**

### 1. TensorBoard monitoring
```bash
# M·ªü TensorBoard ƒë·ªÉ theo d√µi
tensorboard --logdir logs/training

# Ho·∫∑c d√πng Weights & Biases
wandb login
wandb init --project drone-delivery-rl
```

### 2. C√°c metrics ch√≠nh c·∫ßn theo d√µi
| Metric | M·ª•c ti√™u | √ù nghƒ©a |
|--------|----------|---------|
| **Policy Loss** | Gi·∫£m d·∫ßn | M√¥ h√¨nh h·ªçc t·ªët |
| **Value Loss** | Gi·∫£m d·∫ßn | Gi√° tr·ªã ∆∞·ªõc l∆∞·ª£ng ch√≠nh x√°c |
| **Entropy** | ·ªîn ƒë·ªãnh | C√¢n b·∫±ng exploration/exploitation |
| **Episode Reward** | TƒÉng d·∫ßn | Hi·ªáu su·∫•t c·∫£i thi·ªán |
| **Success Rate** | >95% | Nhi·ªám v·ª• ho√†n th√†nh t·ªët |

### 3. Gi√°m s√°t th·ª±c th·ªùi
```bash
# Ki·ªÉm tra log hu·∫•n luy·ªán
tail -f logs/training/training.log

# Ki·ªÉm tra checkpoints
ls -la models/checkpoints/
watch -n 1 'ls -la models/checkpoints/'
```

---

## üî• **TƒÇNG T·ªêC HU·∫§N LUY·ªÜN**

### 1. S·ª≠ d·ª•ng GPU
```bash
# ƒê·∫£m b·∫£o CUDA available
python -c "import torch; print(torch.cuda.is_available())"

# Hu·∫•n luy·ªán v·ªõi GPU (m·∫∑c ƒë·ªãnh)
export CUDA_VISIBLE_DEVICES=0
python scripts/training/train_ppo.py
```

### 2. TƒÉng s·ªë l∆∞·ª£ng m√¥i tr∆∞·ªùng song song
```python
# Trong training script
from src.environment import ParallelDroneEnvironment

# T·∫°o nhi·ªÅu m√¥i tr∆∞·ªùng song song
parallel_env = ParallelDroneEnvironment(
    num_envs=8,  # TƒÉng s·ªë l∆∞·ª£ng m√¥i tr∆∞·ªùng
    config=training_config
)
```

### 3. ƒêi·ªÅu ch·ªânh batch size
```yaml
# TƒÉng batch size n·∫øu c√≥ ƒë·ªß RAM/GPU
ppo:
  batch_size: 128    # Thay v√¨ 64
 rollout_length: 4096  # Thay v√¨ 2048
```

---

## üîÑ **HU·∫§N LUY·ªÜN TI·∫æP T·ª§C (RESUME)**

### 1. Resume t·ª´ checkpoint
```bash
# Resume hu·∫•n luy·ªán t·ª´ checkpoint
python scripts/training/resume_training.py \
    --checkpoint models/checkpoints/ppo_checkpoint_1000000.pt \
    --config config/training/ppo_hyperparameters.yaml \
    --additional-timesteps 4000000
```

### 2. Fine-tuning t·ª´ m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
```bash
# Fine-tuning v·ªõi m√¥i tr∆∞·ªùng m·ªõi
python scripts/training/train_ppo.py \
    --resume models/checkpoints/final_model.pt \
    --config config/training/new_environment_config.yaml \
    --learning-rate 1.0e-5  # Gi·∫£m learning rate
```

---

## üõ°Ô∏è **QU·∫¢N L√ù CHECKPOINTS**

### 1. T·ª± ƒë·ªông l∆∞u checkpoints
```python
# CheckpointManager trong training
from src.rl.utils import CheckpointManager

checkpoint_manager = CheckpointManager(
    save_dir="models/checkpoints",
    save_freq=100000,      # L∆∞u m·ªói 100k timesteps
    max_checkpoints=5,     # Gi·ªØ t·ªëi ƒëa 5 checkpoints
    metric_to_track="success_rate"  # Theo d√µi metric
)
```

### 2. C·∫•u h√¨nh l∆∞u checkpoint
```yaml
# config/training/checkpoint_config.yaml
checkpoint:
  save_frequency: 100000     # Timesteps gi·ªØa c√°c l·∫ßn l∆∞u
  save_best_only: false      # L∆∞u t·∫•t c·∫£ ho·∫∑c ch·ªâ t·ªët nh·∫•t
  save_best_metric: "success_rate"  # Metric ƒë·ªÉ ƒë√°nh gi√° t·ªët nh·∫•t
  keep_checkpoints: 10       # S·ªë l∆∞·ª£ng checkpoints gi·ªØ l·∫°i
 save_optimizer_state: true # C√≥ l∆∞u optimizer kh√¥ng
  save_training_state: true  # C√≥ l∆∞u tr·∫°ng th√°i hu·∫•n luy·ªán kh√¥ng
```

---

## üß™ **KI·ªÇM TRA TRONG HU·∫§N LUY·ªÜN**

### 1. ƒê√°nh gi√° ƒë·ªãnh k·ª≥
```bash
# Ch·∫°y evaluation trong qu√° tr√¨nh training
python scripts/evaluation/evaluate_during_training.py \
    --checkpoint-dir models/checkpoints \
    --eval-freq 50000 \
    --num-episodes 20
```

### 2. Early stopping
```python
# C·∫•u h√¨nh early stopping
early_stopping = {
    'patience': 10,           # S·ªë l∆∞·ª£ng evaluation kh√¥ng c·∫£i thi·ªán
    'min_delta': 0.01,       # C·∫£i thi·ªán t·ªëi thi·ªÉu
    'metric': 'success_rate'  # Metric ƒë·ªÉ theo d√µi
}
```

---

## üìà **PH√ÇN T√çCH K·∫æT QU·∫¢ HU·∫§N LUY·ªÜN**

### 1. Ph√¢n t√≠ch learning curves
```bash
# T·∫°o bi·ªÉu ƒë·ªì h·ªçc t·∫≠p
python scripts/utilities/visualize_training.py \
    --log-dir logs/training \
    --output-dir results/training_curves
```

### 2. Ph√¢n t√≠ch hi·ªáu su·∫•t
```bash
# Ph√¢n t√≠ch chi ti·∫øt hi·ªáu su·∫•t
python scripts/utilities/analyze_training_performance.py \
    --checkpoint models/checkpoints/final_model.pt \
    --metrics-dir logs/training/metrics
```

---

## ‚ö†Ô∏è **L∆ØU √ù QUAN TR·ªåNG**

### 1. Overfitting prevention
```yaml
# C·∫•u h√¨nh regularization
training:
  ppo:
    entropy_coefficient: 0.01    # Gi·ªØ exploration
    clip_range: 0.2             # Tr√°nh c·∫≠p nh·∫≠t qu√° l·ªõn
    max_grad_norm: 0.5          # Gradient clipping
    learning_rate_schedule: "linear"  # Gi·∫£m learning rate
```

### 2. Memory management
```bash
# Gi√°m s√°t memory usage
watch -n 1 'nvidia-smi'  # GPU memory
htop # CPU & RAM usage

# ƒêi·ªÅu ch·ªânh n·∫øu memory kh√¥ng ƒë·ªß
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
```

---

## üö® **X·ª¨ L√ù S·ª∞ C·ªê**

### 1. Hu·∫•n luy·ªán kh√¥ng h·ªôi t·ª•
```bash
# Gi·∫£m learning rate
python scripts/training/train_ppo.py --learning-rate 1.0e-5

# TƒÉng entropy coefficient
python scripts/training/train_ppo.py --entropy-coeff 0.05
```

### 2. Memory overflow
```bash
# Gi·∫£m rollout length
# config/training/ppo_hyperparameters.yaml:
ppo:
  rollout_length: 1024  # Thay v√¨ 2048
  batch_size: 32        # Thay v√¨ 64
```

### 3. Out of memory (CUDA)
```bash
# D√πng CPU thay v√¨ GPU
export CUDA_VISIBLE_DEVICES=""

# Ho·∫∑c tƒÉng virtual memory
sudo swapon --show
```

---

## üèÜ **HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN**

### 1. Ki·ªÉm tra m√¥ h√¨nh cu·ªëi c√πng
```bash
# ƒê√°nh gi√° m√¥ h√¨nh cu·ªëi c√πng
python scripts/evaluation/evaluate_model.py \
    --model models/checkpoints/final_model.pt \
    --episodes 100 \
    --render false
```

### 2. K·∫øt qu·∫£ mong ƒë·ª£i (Table 3)
| Metric | Target | Expected |
|--------|--------|----------|
| **Success Rate** | ‚â•96% | 96.2% |
| **Energy Consumption** | - | 610J |
| **Flight Time** | - | 31.5s |
| **Collision Rate** | ‚â§2% | 0.7% |
| **ATE Error** | ‚â§5cm | 0.8cm |

### 3. T·ªëi ∆∞u m√¥ h√¨nh
```bash
# T·ªëi ∆∞u m√¥ h√¨nh cho inference
python scripts/utilities/optimize_model.py \
    --input models/checkpoints/final_model.pt \
    --output models/optimized/final_model_optimized.pt
```

---

## üìû **H·ªñ TR·ª¢ & T√ÄI NGUY√äN**

### T√†i li·ªáu li√™n quan:
- **Config reference**: config/training/README.md
- **Hyperparameter guide**: docs/HYPERPARAMETER_GUIDE.md  
- **Troubleshooting**: docs/ERROR_HANDLING.md

### C√°c script h·ªØu √≠ch:
- `scripts/training/hyperparameter_search.py` - T√¨m ki·∫øm si√™u tham s·ªë
- `scripts/training/monitor_training.py` - Gi√°m s√°t hu·∫•n luy·ªán
- `scripts/utilities/export_model.py` - Xu·∫•t m√¥ h√¨nh cho deployment

**üéâ M√¥ h√¨nh PPO ƒë√£ s·∫µn s√†ng cho qu√° tr√¨nh hu·∫•n luy·ªán!**